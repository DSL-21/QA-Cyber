import requests
from bs4 import BeautifulSoup
import re
import os
import sys
import time
from urllib.parse import urljoin, urlparse
from collections import deque # Pro spr√°vu fronty URL pro skenov√°n√≠

# --- VA≈†E ASCII ART LOGO ---
ascii_logo = (
    "  ____  ____  __    _  _  _  _  ____ \n"
    " (    \\(  __)(  )  / )( \\( \\/ )(  __)\n"
    "  ) D ( ) _) / (_/\\) \\/ ( )  (  ) _) \n"
    " (____/(____)\\____/\\____/(_/\\_)(____)\n"
    "***************************************\n"
    "* Copyright 2025, ‚òÖDSL‚òÖ               *\n"
    "* https://github.com/DSL-21           *\n"
    "***************************************"
)

# ANSI escape k√≥dy pro barvy a styly v termin√°lu
# \033[1m - Bold (tuƒçn√©)
# \033[0m - Reset (resetuje styl na v√Ωchoz√≠)
# \033[32m - Zelen√° barva (√∫spƒõch/nalezeno)
# \033[33m - ≈Ωlut√° barva (upozornƒõn√≠)
# \033[31m - ƒåerven√° barva (chyba)
# \033[36m - Azurov√° barva (hlaviƒçky)
# \033[35m - Purpurov√° barva (meta tagy)
# \033[34m - Modr√° barva (hloubka/koment√°≈ôe/API/chyby)

# Glob√°ln√≠ seznam pro sledov√°n√≠ nav≈°t√≠ven√Ωch URL, aby se zabr√°nilo nekoneƒçn√Ωm smyƒçk√°m
visited_urls = set()
# Glob√°ln√≠ promƒõnn√° pro crawl-delay
global_crawl_delay = 3 # V√Ωchoz√≠ hodnota

def clear_screen():
    """Vyma≈æe obrazovku termin√°lu."""
    os.system('clear' if os.name == 'posix' else 'cls')

def get_crawl_delay_from_robots(robots_text):
    """
    Analyzuje text robots.txt a pokus√≠ se naj√≠t crawl-delay.
    """
    crawl_delay_match = re.search(r'Crawl-delay:\s*(\d+)', robots_text, re.IGNORECASE)
    if crawl_delay_match:
        return int(crawl_delay_match.group(1))
    return 0 # Nen√≠ explicitnƒõ specifikov√°no

def check_robots_txt(base_url):
    """
    Zkontroluje soubor robots.txt pro danou URL a vyp√≠≈°e jeho obsah.
    """
    global global_crawl_delay
    robots_url = base_url.rstrip('/') + '/robots.txt'
    try:
        response = requests.get(robots_url, timeout=5)
        if response.status_code == 200:
            print(f"\n{os.linesep}\033[1m--- Obsah robots.txt ---\033[0m")
            print(response.text)
            print("------------------------")
            
            delay = get_crawl_delay_from_robots(response.text)
            if delay > 0:
                global_crawl_delay = delay
                print(f"\n\033[33m[*] robots.txt doporuƒçuje Crawl-delay: {global_crawl_delay} sekund.\033[0m")
            else:
                print("\n[*] robots.txt nedoporuƒçuje specifick√Ω Crawl-delay. Pou≈æ√≠v√°m v√Ωchoz√≠ 3 sekundy.")
                global_crawl_delay = 3
            return True
        else:
            print(f"\n[*] robots.txt nenalezen nebo nedostupn√Ω (Status: {response.status_code})")
            global_crawl_delay = 3
            return False
    except requests.exceptions.RequestException as e:
        print(f"\n[*] Nepoda≈ôilo se z√≠skat robots.txt: {e}")
        global_crawl_delay = 3
        return False

def extract_info_from_page(soup, response_text, all_found_data, scan_choices):
    """
    Extrahuje r≈Øzn√© typy informac√≠ z BeautifulSoup objektu a textu odpovƒõdi.
    """
    
    # --- Extrakce e-mailov√Ωch adres ---
    if scan_choices['emails']:
        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        found_emails = set(re.findall(email_pattern, response_text)) # Hled√°me v cel√©m textu odpovƒõdi
        for email in found_emails:
            all_found_data['emails'].add(email)

    # --- Extrakce telefonn√≠ch ƒç√≠sel ---
    if scan_choices['phones']:
        phone_pattern = r'(?:\+\d{1,3}[ -]?)?(?:\(\d{1,4}\)[ -]?)?\d{2,4}[ -]?\d{2,4}[ -]?\d{2,4}(?:[ -]?\d{1,4})?'
        found_phones = set(re.findall(phone_pattern, soup.get_text()))
        
        cleaned_phones = set()
        for phone in found_phones:
            digits = re.sub(r'\D', '', phone)
            if 7 <= len(digits) <= 15:
                cleaned_phones.add(phone.strip())
                
        for phone in cleaned_phones:
            all_found_data['phones'].add(phone)

    # --- Extrakce odkaz≈Ø na soci√°ln√≠ m√©dia ---
    if scan_choices['social_links']:
        social_media_domains = {
            'facebook.com', 'fb.com', 'twitter.com', 'x.com', 'linkedin.com',
            'instagram.com', 'youtube.com', 'tiktok.com', 'pinterest.com'
        }
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            if any(domain in href for domain in social_media_domains):
                all_found_data['social_links'].add(href)

    # --- Extrakce HTML Meta Tag≈Ø ---
    if scan_choices['meta_tags']:
        generator_meta = soup.find('meta', attrs={'name': 'generator'})
        if generator_meta and 'content' in generator_meta.attrs:
            all_found_data['meta_tags']['generator'] = generator_meta['content']
            
        description_meta = soup.find('meta', attrs={'name': 'description'})
        if description_meta and 'content' in description_meta.attrs:
            all_found_data['meta_tags']['description'] = description_meta['content']

        keywords_meta = soup.find('meta', attrs={'name': 'keywords'})
        if keywords_meta and 'content' in keywords_meta.attrs:
            all_found_data['meta_tags']['keywords'] = keywords_meta['content']
                
        og_title = soup.find('meta', property='og:title')
        if og_title and 'content' in og_title.attrs:
            all_found_data['meta_tags']['og_title'] = og_title['content']
        og_description = soup.find('meta', property='og:description')
        if og_description and 'content' in og_description.attrs:
            all_found_data['meta_tags']['og_description'] = og_description['content']

    # --- Extrakce HTML/JS Koment√°≈ô≈Ø ---
    if scan_choices['comments']:
        # Hled√°me HTML koment√°≈ôe
        html_comments = soup.find_all(string=lambda text: isinstance(text, Comment))
        for comment in html_comments:
            all_found_data['comments'].add(comment.strip())
        
        # Hled√°me JS koment√°≈ôe uvnit≈ô <script> tag≈Ø (zjednodu≈°enƒõ)
        for script_tag in soup.find_all('script', string=True):
            script_content = script_tag.string
            # Jednoduch√Ω regex pro // a /* */ koment√°≈ôe v JS
            js_comment_pattern = r'//.*|/\*[\s\S]*?\*/'
            found_js_comments = re.findall(js_comment_pattern, script_content)
            for comment in found_js_comments:
                all_found_data['comments'].add(comment.strip())

    # --- Extrakce potenci√°ln√≠ch API kl√≠ƒç≈Ø/token≈Ø ---
    if scan_choices['api_keys']:
        # Velmi jednoduch√© vzory pro demonstraƒçn√≠ √∫ƒçely.
        # Re√°ln√© API kl√≠ƒçe maj√≠ slo≈æitƒõj≈°√≠ a r≈Øznorodƒõj≈°√≠ form√°ty.
        api_key_patterns = [
            r'(?:api_key|apikey|token|secret|client_id|client_secret)=([a-zA-Z0-9_-]{16,64})', # Obecn√Ω kl√≠ƒç/token
            r'sk-[a-zA-Z0-9]{32,}', # P≈ô√≠klad pro OpenAI-like kl√≠ƒçe
            r'AIza[0-9A-Za-z-_]{35}', # P≈ô√≠klad pro Google API kl√≠ƒçe
            r'pk_live_[a-zA-Z0-9]{24}', # P≈ô√≠klad pro Stripe public keys
        ]
        
        for pattern in api_key_patterns:
            found_keys = re.findall(pattern, response_text)
            for key in found_keys:
                all_found_data['api_keys'].add(key)

    # --- Extrakce bƒõ≈æn√Ωch chybov√Ωch zpr√°v ---
    if scan_choices['error_messages']:
        error_keywords = [
            'SQL Error', 'Fatal Error', 'Warning:', 'Deprecated:', 
            'Stack trace', 'Parse error', 'syntax error', 'uncaught exception',
            'database error', 'connection refused', 'permission denied'
        ]
        page_text = soup.get_text()
        for keyword in error_keywords:
            if re.search(r'\b' + re.escape(keyword) + r'\b', page_text, re.IGNORECASE):
                # Pokus√≠me se zachytit kontext chyby
                match = re.search(r'(.{0,100}' + re.escape(keyword) + r'.{0,100})', page_text, re.IGNORECASE | re.DOTALL)
                if match:
                    all_found_data['error_messages'].add(match.group(1).strip().replace('\n', ' '))
                else:
                    all_found_data['error_messages'].add(keyword)


def get_page_content_and_info(url, all_found_data, scan_choices, current_depth, max_depth):
    """
    St√°hne obsah webov√© str√°nky, extrahuje informace a najde intern√≠ odkazy.
    """
    global visited_urls
    global global_crawl_delay

    if url in visited_urls:
        return [] # Ji≈æ nav≈°t√≠veno, p≈ôeskoƒçit

    visited_urls.add(url) # P≈ôidat do nav≈°t√≠ven√Ωch

    prefix_indent = "  " * (current_depth + 1)
    print(f"\n{prefix_indent}\033[1m--- Skenuji str√°nku (Hloubka {current_depth}): {url} ---\033[0m")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # --- Extrakce HTTP Hlaviƒçek (v≈ædy, proto≈æe jsou z√°kladn√≠) ---
        if scan_choices['http_headers']:
            for header_name in ['Server', 'X-Powered-By', 'Content-Type', 'Date', 
                                'Strict-Transport-Security', 'X-Frame-Options', 
                                'Content-Security-Policy', 'X-XSS-Protection', 'X-Content-Type-Options']:
                if header_name in response.headers:
                    all_found_data['http_headers'][header_name] = response.headers[header_name]

        # Extrahujeme dal≈°√≠ informace na z√°kladƒõ voleb u≈æivatele
        extract_info_from_page(soup, response.text, all_found_data, scan_choices)

        # Hled√°me intern√≠ odkazy pro hlub≈°√≠ skenov√°n√≠, pokud jsme nedos√°hli max_depth
        internal_links = set()
        if current_depth < max_depth:
            base_domain = urlparse(url).netloc
            
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                full_url = urljoin(url, href)
                parsed_full_url = urlparse(full_url)

                # Kontrola, zda je odkaz intern√≠ a nen√≠ soubor
                if parsed_full_url.netloc == base_domain and \
                   not re.search(r'\.(pdf|jpg|jpeg|png|gif|zip|rar|doc|docx|xls|xlsx|ppt|pptx)$', parsed_full_url.path, re.IGNORECASE):
                    # Zde m≈Ø≈æeme p≈ôidat i filtrov√°n√≠ na "d≈Øle≈æit√©" odkazy, nebo skenovat v≈°echny,
                    # pro hlub≈°√≠ skenov√°n√≠ je obvykle lep≈°√≠ skenovat v≈°echny intern√≠ odkazy.
                    if full_url not in visited_urls:
                        internal_links.add(full_url)
                            
        return list(internal_links)
        
    except requests.exceptions.MissingSchema:
        print(f"{prefix_indent}\033[31m‚ùå Chyba: Neplatn√° URL. Ujistƒõte se, ≈æe URL zaƒç√≠n√° 'http://' nebo 'https://'.\033[0m")
    except requests.exceptions.ConnectionError:
        print(f"{prefix_indent}\033[31m‚ùå Chyba p≈ôipojen√≠: Nelze se p≈ôipojit k webov√© str√°nce. Zkontrolujte URL a p≈ôipojen√≠ k internetu.\033[0m")
    except requests.exceptions.Timeout:
        print(f"{prefix_indent}\033[31m‚ùå Chyba ƒçasov√©ho limitu: Po≈æadavek na webovou str√°nku vypr≈°el.\033[0m")
    except requests.exceptions.RequestException as e:
        print(f"{prefix_indent}\033[31m‚ùå Chyba HTTP po≈æadavku: {e}\033[0m")
    except Exception as e:
        print(f"{prefix_indent}\033[31m‚ùå Vyskytla se neoƒçek√°van√° chyba: {e}\033[0m")
    
    return []

def display_results(all_found_data, scan_choices):
    """
    Zobraz√≠ v≈°echny shrom√°≈ædƒõn√© informace na z√°kladƒõ voleb u≈æivatele.
    """
    print(f"\n{os.linesep}\033[1m--- SHRNUT√ç NALEZEN√ùCH INFORMAC√ç ---\033[0m")

    if scan_choices['emails']:
        print(f"\n{os.linesep}\033[1müìß Nalezen√© e-mailov√© adresy ({len(all_found_data['emails'])}) ---\033[0m")
        if all_found_data['emails']:
            for email in sorted(list(all_found_data['emails'])):
                print(f"  \033[32m- {email}\033[0m")
        else:
            print("  ≈Ω√°dn√© e-mailov√© adresy nenalezeny.")

    if scan_choices['phones']:
        print(f"\n{os.linesep}\033[1müìû Nalezen√° telefonn√≠ ƒç√≠sla ({len(all_found_data['phones'])}) ---\033[0m")
        if all_found_data['phones']:
            for phone in sorted(list(all_found_data['phones'])):
                print(f"  \033[32m- {phone}\033[0m")
        else:
            print("  ≈Ω√°dn√° telefonn√≠ ƒç√≠sla nenalezena.")

    if scan_choices['social_links']:
        print(f"\n{os.linesep}\033[1müîó Nalezen√© odkazy na soci√°ln√≠ m√©dia ({len(all_found_data['social_links'])}) ---\033[0m")
        if all_found_data['social_links']:
            for link in sorted(list(all_found_data['social_links'])):
                print(f"  \033[32m- {link}\033[0m")
        else:
            print("  ≈Ω√°dn√© odkazy na soci√°ln√≠ m√©dia nenalezeny.")

    if scan_choices['http_headers']:
        print(f"\n{os.linesep}\033[1m--- Unik√°tn√≠ HTTP Hlaviƒçky ---\033[0m")
        if all_found_data['http_headers']:
            for header_name, header_value in sorted(all_found_data['http_headers'].items()):
                print(f"  \033[36m{header_name}:\033[0m {header_value}")
        else:
            print("  ≈Ω√°dn√© specifick√© HTTP hlaviƒçky nenalezeny.")

    if scan_choices['meta_tags']:
        print(f"\n{os.linesep}\033[1m--- Unik√°tn√≠ HTML Meta Tagy ---\033[0m")
        if all_found_data['meta_tags']:
            for meta_name, meta_value in sorted(all_found_data['meta_tags'].items()):
                print(f"  \033[35m{meta_name}:\033[0m {meta_value}")
        else:
            print("  ≈Ω√°dn√© specifick√© meta tagy nenalezeny.")
            
    if scan_choices['comments']:
        print(f"\n{os.linesep}\033[1m--- Nalezen√© HTML/JS Koment√°≈ôe ({len(all_found_data['comments'])}) ---\033[0m")
        if all_found_data['comments']:
            for comment in sorted(list(all_found_data['comments'])):
                print(f"  \033[34m- {comment}\033[0m")
        else:
            print("  ≈Ω√°dn√© koment√°≈ôe nenalezeny.")

    if scan_choices['api_keys']:
        print(f"\n{os.linesep}\033[1m--- Potenci√°ln√≠ API kl√≠ƒçe/tokeny ({len(all_found_data['api_keys'])}) ---\033[0m")
        if all_found_data['api_keys']:
            print("\033[33m  Upozornƒõn√≠: Toto jsou vzory. Nemus√≠ j√≠t o aktivn√≠ kl√≠ƒçe. Pro vzdƒõl√°vac√≠ √∫ƒçely.\033[0m")
            for key in sorted(list(all_found_data['api_keys'])):
                print(f"  \033[34m- {key}\033[0m")
        else:
            print("  ≈Ω√°dn√© potenci√°ln√≠ API kl√≠ƒçe/tokeny nenalezeny.")

    if scan_choices['error_messages']:
        print(f"\n{os.linesep}\033[1m--- Nalezen√© chybov√© zpr√°vy ({len(all_found_data['error_messages'])}) ---\033[0m")
        if all_found_data['error_messages']:
            print("\033[33m  Upozornƒõn√≠: Nalezen√© chybov√© zpr√°vy mohou naznaƒçovat probl√©my, ale nejsou v≈ædy zranitelnost√≠.\033[0m")
            for msg in sorted(list(all_found_data['error_messages'])):
                print(f"  \033[34m- {msg}\033[0m")
        else:
            print("  ≈Ω√°dn√© chybov√© zpr√°vy nenalezeny.")
        
    print("\n" + "=" * (os.get_terminal_size().columns - 1) if os.get_terminal_size().columns > 1 else "=")


def main():
    """Hlavn√≠ funkce pro spu≈°tƒõn√≠ OSINT Harvesteru."""
    global visited_urls
    global global_crawl_delay

    try:
        while True:
            clear_screen()
            print(ascii_logo)
            print("\n--- OSINT Harvester (Komplexn√≠ pr≈Øzkum) ---")
            print("Zadejte URL webov√© str√°nky ke skenov√°n√≠ (nap≈ô. 'https://example.com').")
            print("Pro ukonƒçen√≠ zadejte 'konec'.")
            print("\n\033[31m!!! Pamatujte na etick√© a pr√°vn√≠ z√°sady web scrapingu !!!\033[0m")
            print("\033[31m!!! Scrapujte POUZE ve≈ôejnƒõ dostupn√° data a respektujte robots.txt a ToS !!!\033[0m")
            
            target_url = input("\nZadejte c√≠lovou URL: ").strip()
            
            if target_url.lower() == 'konec':
                print("Ukonƒçuji OSINT Harvester. Na shledanou!")
                break
            
            if not target_url:
                print("C√≠lov√° URL nem≈Ø≈æe b√Ωt pr√°zdn√°. Zkuste to znovu.")
                input("\nStisknƒõte Enter pro pokraƒçov√°n√≠...")
                continue

            # Zkontrolujeme, zda URL zaƒç√≠n√° http/https
            if not target_url.startswith(('http://', 'https://')):
                target_url = 'https://' + target_url
                print(f"[*] URL upravena na: {target_url}")

            # --- Interaktivn√≠ volba typ≈Ø informac√≠ ---
            scan_choices = {
                'emails': input("Hledat e-mailov√© adresy? (a/n): ").lower().startswith('a'),
                'phones': input("Hledat telefonn√≠ ƒç√≠sla? (a/n): ").lower().startswith('a'),
                'social_links': input("Hledat odkazy na soci√°ln√≠ m√©dia? (a/n): ").lower().startswith('a'),
                'http_headers': input("Analyzovat HTTP hlaviƒçky? (a/n): ").lower().startswith('a'),
                'meta_tags': input("Analyzovat HTML meta tagy? (a/n): ").lower().startswith('a'),
                'comments': input("Hledat HTML/JS koment√°≈ôe? (a/n): ").lower().startswith('a'),
                'api_keys': input("Hledat potenci√°ln√≠ API kl√≠ƒçe/tokeny? (a/n): ").lower().startswith('a'),
                'error_messages': input("Hledat chybov√© zpr√°vy? (a/n): ").lower().startswith('a')
            }
            
            # --- Nastaviteln√° hloubka skenov√°n√≠ ---
            while True:
                try:
                    max_depth_input = input("Zadejte maxim√°ln√≠ hloubku skenov√°n√≠ (0 pro jen hlavn√≠ str√°nku, 1 pro hlavn√≠ + 1 √∫rove≈à, atd.): ")
                    max_depth = int(max_depth_input)
                    if max_depth < 0:
                        print("\033[31mHloubka nem≈Ø≈æe b√Ωt z√°porn√°. Zadejte pros√≠m nez√°porn√© ƒç√≠slo.\033[0m")
                    else:
                        break
                except ValueError:
                    print("\033[31mNeplatn√Ω vstup. Zadejte pros√≠m ƒç√≠slo.\033[0m")

            # Resetujeme nav≈°t√≠ven√© URL a data pro ka≈æd√© nov√© skenov√°n√≠ c√≠le
            visited_urls.clear() 
            all_found_data = {
                'emails': set(),
                'phones': set(),
                'social_links': set(),
                'http_headers': {},
                'meta_tags': {},
                'comments': set(),
                'api_keys': set(),
                'error_messages': set()
            }

            # Nejprve zkontrolujeme robots.txt pro hlavn√≠ URL
            check_robots_txt(target_url)

            # Pou≈æ√≠v√°me frontu pro BFS (Breadth-First Search) pro hlub≈°√≠ skenov√°n√≠
            # Ka≈æd√° polo≈æka ve frontƒõ je (url, depth)
            urls_to_visit = deque([(target_url, 0)])

            print(f"\n\033[33m[*] Spou≈°t√≠m hlubok√© skenov√°n√≠ do hloubky {max_depth}...\033[0m")

            while urls_to_visit:
                current_url, current_depth = urls_to_visit.popleft()

                if current_depth > max_depth:
                    continue # P≈ôeskoƒçit, pokud jsme p≈ôekroƒçili maxim√°ln√≠ hloubku

                # Z√≠skejte obsah str√°nky a extrahujte informace
                # get_page_content_and_info nyn√≠ vrac√≠ intern√≠ odkazy
                new_internal_links = get_page_content_and_info(current_url, all_found_data, scan_choices, current_depth, max_depth)
                
                # P≈ôid√°me nov√© intern√≠ odkazy do fronty, pokud jsme nedos√°hli max_depth
                if current_depth < max_depth:
                    for link in new_internal_links:
                        if link not in visited_urls: # P≈ôid√°me jen ty, kter√© jsme je≈°tƒõ nenav≈°t√≠vili
                            urls_to_visit.append((link, current_depth + 1))
                
                # Dodr≈æujeme crawl-delay po ka≈æd√©m po≈æadavku
                print(f"\n\033[33m[*] ƒåek√°m {global_crawl_delay} sekund...\033[0m")
                time.sleep(global_crawl_delay)

            # Zobraz√≠me souhrn v≈°ech shrom√°≈ædƒõn√Ωch dat
            display_results(all_found_data, scan_choices)
            
            input("\nStisknƒõte Enter pro skenov√°n√≠ dal≈°√≠ URL, nebo 'konec'...")

    except KeyboardInterrupt:
        print("\nProgram byl ukonƒçen u≈æivatelem.")
    except Exception as e:
        print(f"\n\033[31m‚ùå Vyskytla se kritick√° chyba: {e}\033[0m")
    finally:
        sys.stdout.write("\033[0m\n")
        sys.stdout.flush()

if __name__ == "__main__":
    from bs4 import Comment # Importujeme Comment pro detekci HTML koment√°≈ô≈Ø
    main()

