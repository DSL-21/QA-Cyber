import requests
from bs4 import BeautifulSoup, Comment # Import Comment pro detekci HTML koment√°≈ô≈Ø
import re
import os
import sys
import time
from urllib.parse import urljoin, urlparse
from collections import deque # Pro spr√°vu fronty URL pro skenov√°n√≠
import socket # Pro z√≠sk√°n√≠ IP adresy
import json # Pro zpracov√°n√≠ JSON z Wayback Machine API

# --- VA≈†E ASCII ART LOGO ---
ascii_logo = (
    "  ____  ____  __    _  _  _  _  ____ \n"
    " (    \\(  __)(  )  / )( \\( \\/ )(  __)\n"
    "  ) D ( ) _) / (_/\\) \\/ ( )  (  ) _) \n"
    " (____/(____)\\____/\\____/(_/\\_)(____)\n"
    "***************************************\n"
    "* Copyright 2025, ‚òÖDSL‚òÖ               *\n"
    "* https://github.com/DSL-21           *\n"
    "***************************************"
)

# ANSI escape k√≥dy pro barvy a styly v termin√°lu
# \033[1m - Bold (tuƒçn√©)
# \033[0m - Reset (resetuje styl na v√Ωchoz√≠)
# \033[32m - Zelen√° barva (√∫spƒõch/nalezeno)
# \033[33m - ≈Ωlut√° barva (upozornƒõn√≠)
# \033[31m - ƒåerven√° barva (chyba)
# \033[36m - Azurov√° barva (hlaviƒçky)
# \033[35m - Purpurov√° barva (meta tagy)
# \033[34m - Modr√° barva (hloubka/koment√°≈ôe/API/chyby)
# \033[38;5;208m - Oran≈æov√° barva (technologie)
# \033[38;5;198m - R≈Ø≈æov√° barva (extern√≠ OSINT)

# Glob√°ln√≠ seznam pro sledov√°n√≠ nav≈°t√≠ven√Ωch URL, aby se zabr√°nilo nekoneƒçn√Ωm smyƒçk√°m
visited_urls = set()
# Glob√°ln√≠ promƒõnn√° pro crawl-delay
global_crawl_delay = 3 # V√Ωchoz√≠ hodnota

def clear_screen():
    """Vyma≈æe obrazovku termin√°lu."""
    os.system('clear' if os.name == 'posix' else 'cls')

def get_crawl_delay_from_robots(robots_text):
    """
    Analyzuje text robots.txt a pokus√≠ se naj√≠t crawl-delay.
    """
    crawl_delay_match = re.search(r'Crawl-delay:\s*(\d+)', robots_text, re.IGNORECASE)
    if crawl_delay_match:
        return int(crawl_delay_match.group(1))
    return 0 # Nen√≠ explicitnƒõ specifikov√°no

def check_robots_txt(base_url):
    """
    Zkontroluje soubor robots.txt pro danou URL a vyp√≠≈°e jeho obsah.
    """
    global global_crawl_delay
    robots_url = base_url.rstrip('/') + '/robots.txt'
    try:
        response = requests.get(robots_url, timeout=5)
        if response.status_code == 200:
            print(f"\n{os.linesep}\033[1m--- Obsah robots.txt ---\033[0m")
            print(response.text)
            print("------------------------")
            
            delay = get_crawl_delay_from_robots(response.text)
            if delay > 0:
                global_crawl_delay = delay
                print(f"\n\033[33m[*] robots.txt doporuƒçuje Crawl-delay: {global_crawl_delay} sekund.\033[0m")
            else:
                print("\n[*] robots.txt nedoporuƒçuje specifick√Ω Crawl-delay. Pou≈æ√≠v√°m v√Ωchoz√≠ 3 sekundy.")
                global_crawl_delay = 3
            return True
        else:
            print(f"\n[*] robots.txt nenalezen nebo nedostupn√Ω (Status: {response.status_code})")
            global_crawl_delay = 3
            return False
    except requests.exceptions.RequestException as e:
        print(f"\n[*] Nepoda≈ôilo se z√≠skat robots.txt: {e}")
        global_crawl_delay = 3
        return False

def identify_technologies(url, response_headers, soup, response_text, all_found_data):
    """
    Identifikuje webov√© technologie na z√°kladƒõ hlaviƒçek, HTML obsahu a URL.
    """
    
    # Z HTTP hlaviƒçek
    server_header = response_headers.get('Server')
    if server_header:
        all_found_data['technologies']['Web Server'] = server_header
    
    x_powered_by_header = response_headers.get('X-Powered-By')
    if x_powered_by_header:
        all_found_data['technologies']['Powered By'] = x_powered_by_header

    # Z meta tag≈Ø (ji≈æ sb√≠r√°me)
    if 'generator' in all_found_data['meta_tags']:
        all_found_data['technologies']['CMS/Generator'] = all_found_data['meta_tags']['generator']

    # Z HTML obsahu a URL (pro CMS a JS Frameworky)
    
    # WordPress detekce a verze
    wp_version_match = re.search(r'wp-includes/js/dist/vendor/wp-polyfill-fetch\.min\.js\?ver=([0-9.]+)', response_text)
    if wp_version_match:
        all_found_data['technologies']['CMS'] = f'WordPress {wp_version_match.group(1)}'
    elif soup.find('link', rel='stylesheet', href=re.compile(r'/wp-(content|includes)/')) or \
         soup.find('script', src=re.compile(r'/wp-(includes|content)/')):
        all_found_data['technologies']['CMS'] = 'WordPress (verze nezji≈°tƒõna)'
    
    # Joomla detekce a verze
    joomla_meta = soup.find('meta', attrs={'name': 'generator', 'content': re.compile(r'Joomla! (\d+\.\d+\.\d+)', re.IGNORECASE)})
    if joomla_meta:
        all_found_data['technologies']['CMS'] = f"Joomla! {joomla_meta['content']}"
    elif soup.find('script', src=re.compile(r'/media/system/js/joomla.js')):
        all_found_data['technologies']['CMS'] = 'Joomla! (verze nezji≈°tƒõna)'

    # Drupal detekce a verze
    drupal_meta = soup.find('meta', attrs={'name': 'generator', 'content': re.compile(r'Drupal (\d+\.\d+)', re.IGNORECASE)})
    if drupal_meta:
        all_found_data['technologies']['CMS'] = f"Drupal {drupal_meta['content']}"
    elif soup.find('link', href=re.compile(r'/sites/default/files/')):
        all_found_data['technologies']['CMS'] = 'Drupal (verze nezji≈°tƒõna)'

    # Cloudflare CDN
    if 'cloudflare' in response_headers.get('Server', '').lower() or \
       '__cf_bm' in response_headers.get('Set-Cookie', ''):
        all_found_data['technologies']['CDN'] = 'Cloudflare'

    # Google Analytics
    if re.search(r'ga\.js|analytics\.js|gtag\.js|googletagmanager\.com/gtag/js', response_text):
        all_found_data['technologies']['Analytics'] = 'Google Analytics'

    # Google Fonts
    if soup.find('link', href=re.compile(r'fonts\.googleapis\.com|fonts\.gstatic\.com')):
        all_found_data['technologies']['Fonts'] = 'Google Fonts'

    # Font Awesome
    if soup.find('link', href=re.compile(r'fontawesome\.com')) or \
       soup.find('i', class_=re.compile(r'fa[srlb]? fa-')): # Hled√° ikony Font Awesome
        all_found_data['technologies']['Icons'] = 'Font Awesome'

    # jQuery
    if re.search(r'jQuery|jquery\.js', response_text, re.IGNORECASE):
        # Pokus o detekci verze jQuery z k√≥du
        jquery_version_match = re.search(r'jQuery JavaScript Library v([0-9.]+)', response_text)
        if jquery_version_match:
            all_found_data['technologies']['JavaScript Library'] = f'jQuery {jquery_version_match.group(1)}'
        else:
            all_found_data['technologies']['JavaScript Library'] = 'jQuery'

    # React (z√°kladn√≠ detekce)
    if re.search(r'ReactDOM|react-dom\.production\.min\.js|data-reactroot', response_text):
        all_found_data['technologies']['JavaScript Framework'] = 'React'

    # Vue.js (z√°kladn√≠ detekce)
    if re.search(r'Vue\.js|vue\.min\.js|id="app"|data-v-', response_text):
        all_found_data['technologies']['JavaScript Framework'] = 'Vue.js'

    # Angular (z√°kladn√≠ detekce)
    if re.search(r'ng-app|angular\.js|data-ng-app', response_text):
        all_found_data['technologies']['JavaScript Framework'] = 'Angular'


def extract_info_from_page(url, response_headers, soup, response_text, all_found_data, scan_choices):
    """
    Extrahuje r≈Øzn√© typy informac√≠ z BeautifulSoup objektu a textu odpovƒõdi.
    """
    
    # --- Extrakce e-mailov√Ωch adres ---
    if scan_choices['emails']:
        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        found_emails = set(re.findall(email_pattern, response_text))
        for email in found_emails:
            all_found_data['emails'].add(email)

    # --- Extrakce telefonn√≠ch ƒç√≠sel ---
    if scan_choices['phones']:
        # Vylep≈°en√Ω regex pro telefonn√≠ ƒç√≠sla: zahrnuje v√≠ce mezin√°rodn√≠ch p≈ôedpon a form√°t≈Ø
        # P≈ôid√°no v√≠ce variant oddƒõlovaƒç≈Ø, voliteln√© p≈ôedvolby zemƒõ
        phone_pattern = r'(?:\+\d{1,4}[ -]?)?(?:\(\d{1,4}\)[ -]?)?\d{1,4}[ -]?\d{1,4}[ -]?\d{1,4}(?:[ -]?\d{1,4})?'
        found_phones = set(re.findall(phone_pattern, soup.get_text()))
        
        cleaned_phones = set()
        for phone in found_phones:
            digits = re.sub(r'\D', '', phone) # Odstran√≠ v≈°e kromƒõ ƒç√≠slic
            # Filtrovat ƒç√≠sla, kter√° jsou p≈ô√≠li≈° kr√°tk√° nebo p≈ô√≠li≈° dlouh√°
            if 7 <= len(digits) <= 15: # Typick√° d√©lka telefonn√≠ch ƒç√≠sel
                cleaned_phones.add(phone.strip())
                
        for phone in cleaned_phones:
            all_found_data['phones'].add(phone)

    # --- Extrakce odkaz≈Ø na soci√°ln√≠ m√©dia ---
    if scan_choices['social_links']:
        social_media_domains = {
            'facebook.com', 'fb.com', 'twitter.com', 'x.com', 'linkedin.com',
            'instagram.com', 'youtube.com', 'tiktok.com', 'pinterest.com'
        }
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            if any(domain in href for domain in social_media_domains):
                all_found_data['social_links'].add(href)

    # --- Extrakce HTML Meta Tag≈Ø ---
    if scan_choices['meta_tags']:
        generator_meta = soup.find('meta', attrs={'name': 'generator'})
        if generator_meta and 'content' in generator_meta.attrs:
            all_found_data['meta_tags']['generator'] = generator_meta['content']
            
        description_meta = soup.find('meta', attrs={'name': 'description'})
        if description_meta and 'content' in description_meta.attrs:
            all_found_data['meta_tags']['description'] = description_meta['content']

        keywords_meta = soup.find('meta', attrs={'name': 'keywords'})
        if keywords_meta and 'content' in keywords_meta.attrs:
            all_found_data['meta_tags']['keywords'] = keywords_meta['content']
                
        og_title = soup.find('meta', property='og:title')
        if og_title and 'content' in og_title.attrs:
            all_found_data['meta_tags']['og_title'] = og_title['content']
        og_description = soup.find('meta', property='og:description')
        if og_description and 'content' in og_description.attrs:
            all_found_data['meta_tags']['og_description'] = og_description['content']

    # --- Extrakce HTML/JS Koment√°≈ô≈Ø ---
    if scan_choices['comments']:
        html_comments = soup.find_all(string=lambda text: isinstance(text, Comment))
        for comment in html_comments:
            all_found_data['comments'].add(comment.strip())
        
        for script_tag in soup.find_all('script', string=True):
            script_content = script_tag.string
            js_comment_pattern = r'//.*|/\*[\s\S]*?\*/'
            found_js_comments = re.findall(js_comment_pattern, script_content)
            for comment in found_js_comments:
                all_found_data['comments'].add(comment.strip())

    # --- Extrakce potenci√°ln√≠ch API kl√≠ƒç≈Ø/token≈Ø ---
    if scan_choices['api_keys']:
        api_key_patterns = [
            r'(?:api_key|apikey|token|secret|client_id|client_secret)=([a-zA-Z0-9_-]{16,64})',
            r'sk-[a-zA-Z0-9]{32,}',
            r'AIza[0-9A-Za-z-_]{35}',
            r'pk_live_[a-zA-Z0-9]{24}',
        ]
        for pattern in api_key_patterns:
            found_keys = re.findall(pattern, response_text)
            for key in found_keys:
                all_found_data['api_keys'].add(key)

    # --- Extrakce bƒõ≈æn√Ωch chybov√Ωch zpr√°v ---
    if scan_choices['error_messages']:
        error_keywords = [
            'SQL Error', 'Fatal Error', 'Warning:', 'Deprecated:', 
            'Stack trace', 'Parse error', 'syntax error', 'uncaught exception',
            'database error', 'connection refused', 'permission denied'
        ]
        page_text = soup.get_text()
        for keyword in error_keywords:
            if re.search(r'\b' + re.escape(keyword) + r'\b', page_text, re.IGNORECASE):
                match = re.search(r'(.{0,100}' + re.escape(keyword) + r'.{0,100})', page_text, re.IGNORECASE | re.DOTALL)
                if match:
                    all_found_data['error_messages'].add(match.group(1).strip().replace('\n', ' '))
                else:
                    all_found_data['error_messages'].add(keyword)


def get_page_content_and_info(url, all_found_data, scan_choices, current_depth, max_depth):
    """
    St√°hne obsah webov√© str√°nky, extrahuje informace a najde intern√≠ odkazy.
    """
    global visited_urls
    global global_crawl_delay

    if url in visited_urls:
        return [] # Ji≈æ nav≈°t√≠veno, p≈ôeskoƒçit

    visited_urls.add(url) # P≈ôidat do nav≈°t√≠ven√Ωch

    prefix_indent = "  " * (current_depth + 1)
    # Zmƒõnƒõno pro struƒçnƒõj≈°√≠ v√Ωstup bƒõhem skenov√°n√≠
    print(f"{prefix_indent}\033[34m[*] Zpracov√°v√°m: {url} (Hloubka {current_depth})\033[0m")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # --- Extrakce HTTP Hlaviƒçek (v≈ædy, pokud je volba zapnut√°) ---
        if scan_choices['http_headers']:
            for header_name in ['Server', 'X-Powered-By', 'Content-Type', 'Date', 
                                'Strict-Transport-Security', 'X-Frame-Options', 
                                'Content-Security-Policy', 'X-XSS-Protection', 'X-Content-Type-Options']:
                if header_name in response.headers:
                    all_found_data['http_headers'][header_name] = response.headers[header_name]

        # Extrahujeme dal≈°√≠ informace na z√°kladƒõ voleb u≈æivatele
        identify_technologies(url, response.headers, soup, response.text, all_found_data)
        extract_info_from_page(url, response.headers, soup, response.text, all_found_data, scan_choices)

        # Hled√°me intern√≠ odkazy pro hlub≈°√≠ skenov√°n√≠, pokud jsme nedos√°hli max_depth
        internal_links = set()
        if current_depth < max_depth:
            base_domain = urlparse(url).netloc
            
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                full_url = urljoin(url, href)
                parsed_full_url = urlparse(full_url)

                # Kontrola, zda je odkaz intern√≠ a nen√≠ soubor (roz≈°√≠≈ôeno o .css, .js, .xml, .txt, .json)
                if parsed_full_url.netloc == base_domain and \
                   not re.search(r'\.(pdf|jpg|jpeg|png|gif|zip|rar|doc|docx|xls|xlsx|ppt|pptx|css|js|xml|txt|json)$', parsed_full_url.path, re.IGNORECASE):
                    if full_url not in visited_urls:
                        internal_links.add(full_url)
                            
        return list(internal_links)
        
    except requests.exceptions.MissingSchema:
        print(f"{prefix_indent}\033[31m‚ùå Chyba: Neplatn√° URL. Ujistƒõte se, ≈æe URL zaƒç√≠n√° 'http://' nebo 'https://'.\033[0m")
    except requests.exceptions.ConnectionError:
        print(f"{prefix_indent}\033[31m‚ùå Chyba p≈ôipojen√≠: Nelze se p≈ôipojit k webov√© str√°nce. Zkontrolujte URL a p≈ôipojen√≠ k internetu.\033[0m")
    except requests.exceptions.Timeout:
        print(f"{prefix_indent}\033[31m‚ùå Chyba ƒçasov√©ho limitu: Po≈æadavek na webovou str√°nku vypr≈°el.\033[0m")
    except requests.exceptions.RequestException as e:
        print(f"{prefix_indent}\033[31m‚ùå Chyba HTTP po≈æadavku: {e}\033[0m")
    except Exception as e:
        print(f"{prefix_indent}\033[31m‚ùå Vyskytla se neoƒçek√°van√° chyba: {e}\033[0m")
    
    return []

def get_subdomains_from_crtsh(domain):
    """
    Z√≠sk√° subdom√©ny z crt.sh (Certificate Transparency logs).
    """
    subdomains = set()
    url = f"https://crt.sh/?q=%25.{domain}&output=json"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        data = json.loads(response.text)
        
        for entry in data:
            if 'common_name' in entry:
                cn = entry['common_name']
                if cn.endswith(f".{domain}") or cn == domain:
                    # Odstranit wildcard subdom√©ny jako *.example.com
                    if not cn.startswith('*.'):
                        subdomains.add(cn)
            if 'name_value' in entry:
                names = entry['name_value'].split('\n')
                for name in names:
                    if (name.endswith(f".{domain}") or name == domain) and not name.startswith('*.'):
                        subdomains.add(name)
        return sorted(list(subdomains))
    except json.JSONDecodeError:
        print(f"\033[31m[-] Chyba dek√≥dov√°n√≠ JSON z crt.sh pro {domain}. Mo≈æn√° ≈æ√°dn√© v√Ωsledky nebo zmƒõna form√°tu.\033[0m")
    except requests.exceptions.RequestException as e:
        print(f"\033[31m[-] Chyba p≈ôi z√≠sk√°v√°n√≠ subdom√©n z crt.sh pro {domain}: {e}\033[0m")
    return []

def get_reverse_ip_from_hackertarget(domain, all_found_data):
    """
    Z√≠sk√° dom√©ny hostovan√© na stejn√© IP adrese z HackerTarget.com API.
    """
    reverse_ip_domains = set()
    ip_address = None
    try:
        ip_address = socket.gethostbyname(domain)
        all_found_data['target_ip'] = ip_address # Ulo≈æit IP adresu
        print(f"\033[34m[*] IP adresa pro {domain}: {ip_address}\033[0m")
    except socket.gaierror:
        print(f"\033[31m[-] Nelze z√≠skat IP adresu pro {domain}.\033[0m")
        return []

    if not ip_address:
        return []

    url = f"https://api.hackertarget.com/reverseiplookup/?q={ip_address}"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status() # Vyvol√° v√Ωjimku pro HTTP chyby (4xx nebo 5xx)
        
        # HackerTarget API vrac√≠ prost√Ω text, jedna dom√©na na ≈ô√°dek
        domains_text = response.text.strip()
        if domains_text and "error" not in domains_text.lower() and "no records found" not in domains_text.lower():
            for line in domains_text.splitlines():
                domain_name = line.strip()
                if domain_name:
                    reverse_ip_domains.add(domain_name)
        
        return sorted(list(reverse_ip_domains))
    except requests.exceptions.RequestException as e:
        print(f"\033[31m[-] Chyba p≈ôi z√≠sk√°v√°n√≠ reverzn√≠ch IP dom√©n z HackerTarget.com pro {ip_address}: {e}\033[0m")
    return []

def get_wayback_machine_archives(url_to_archive):
    """
    Z√≠sk√° archivovan√© URL z Wayback Machine (CDX API).
    """
    archives = []
    # CDX API pro z√≠sk√°n√≠ v≈°ech sn√≠mk≈Ø pro danou URL
    # filter=statuscode:200 - pouze √∫spƒõ≈°n√© sn√≠mky
    # limit=100 - omez√≠me poƒçet v√Ωsledk≈Ø pro p≈ôehlednost
    cdx_url = f"http://web.archive.org/cdx/search/cdx?url={url_to_archive}/*&output=json&filter=statuscode:200&limit=100"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

    try:
        response = requests.get(cdx_url, headers=headers, timeout=15)
        response.raise_for_status()
        data = json.loads(response.text)
        
        # Prvn√≠ ≈ô√°dek je hlaviƒçka, zbytek jsou data
        if data and len(data) > 1:
            headers = data[0]
            for entry in data[1:]:
                record = dict(zip(headers, entry))
                if record.get('timestamp') and record.get('original'):
                    archive_url = f"https://web.archive.org/web/{record['timestamp']}/{record['original']}"
                    archives.append({
                        'timestamp': record['timestamp'],
                        'original_url': record['original'],
                        'archive_url': archive_url
                    })
        return archives
    except json.JSONDecodeError:
        print(f"\033[31m[-] Chyba dek√≥dov√°n√≠ JSON z Wayback Machine pro {url_to_archive}. Mo≈æn√° ≈æ√°dn√© v√Ωsledky nebo zmƒõna form√°tu.\033[0m")
    except requests.exceptions.RequestException as e:
        print(f"\033[31m[-] Chyba p≈ôi z√≠sk√°v√°n√≠ archiv≈Ø z Wayback Machine pro {url_to_archive}: {e}\033[0m")
    return []


def display_results(all_found_data, scan_choices, display_limits):
    """
    Zobraz√≠ v≈°echny shrom√°≈ædƒõn√© informace na z√°kladƒõ voleb u≈æivatele a limit≈Ø zobrazen√≠.
    """
    print(f"\n{os.linesep}\033[1m--- SHRNUT√ç NALEZEN√ùCH INFORMAC√ç ---\033[0m")

    if scan_choices['emails']:
        print(f"\n{os.linesep}\033[1müìß Nalezen√© e-mailov√© adresy ({len(all_found_data['emails'])}) ---\033[0m")
        if all_found_data['emails']:
            for email in sorted(list(all_found_data['emails'])):
                print(f"  \033[32m- {email}\033[0m")
        else:
            print("  ≈Ω√°dn√© e-mailov√© adresy nenalezeny.")

    if scan_choices['phones']:
        print(f"\n{os.linesep}\033[1müìû Nalezen√° telefonn√≠ ƒç√≠sla ({len(all_found_data['phones'])}) ---\033[0m")
        if all_found_data['phones']:
            for phone in sorted(list(all_found_data['phones'])):
                print(f"  \033[32m- {phone}\033[0m")
        else:
            print("  ≈Ω√°dn√° telefonn√≠ ƒç√≠sla nenalezena.")

    if scan_choices['social_links']:
        print(f"\n{os.linesep}\033[1müîó Nalezen√© odkazy na soci√°ln√≠ m√©dia ({len(all_found_data['social_links'])}) ---\033[0m")
        if all_found_data['social_links']:
            for link in sorted(list(all_found_data['social_links'])):
                print(f"  \033[32m- {link}\033[0m")
        else:
            print("  ≈Ω√°dn√© odkazy na soci√°ln√≠ m√©dia nenalezeny.")

    if scan_choices['http_headers']:
        print(f"\n{os.linesep}\033[1m--- Unik√°tn√≠ HTTP Hlaviƒçky ---\033[0m")
        if all_found_data['http_headers']:
            for header_name, header_value in sorted(all_found_data['http_headers'].items()):
                print(f"  \033[36m{header_name}:\033[0m {header_value}")
        else:
            print("  ≈Ω√°dn√© specifick√© HTTP hlaviƒçky nenalezeny.")

    if scan_choices['meta_tags']:
        print(f"\n{os.linesep}\033[1m--- Unik√°tn√≠ HTML Meta Tagy ---\033[0m")
        if all_found_data['meta_tags']:
            for meta_name, meta_value in sorted(all_found_data['meta_tags'].items()):
                print(f"  \033[35m{meta_name}:\033[0m {meta_value}")
        else:
            print("  ≈Ω√°dn√© specifick√© meta tagy nenalezeny.")
            
    if scan_choices['comments']:
        print(f"\n{os.linesep}\033[1m--- Nalezen√© HTML/JS Koment√°≈ôe ({len(all_found_data['comments'])}) ---\033[0m")
        if all_found_data['comments']:
            for i, comment in enumerate(sorted(list(all_found_data['comments']))):
                if display_limits['comments'] != 0 and i >= display_limits['comments']:
                    print(f"  \033[34m- ... a dal≈°√≠ch {len(all_found_data['comments']) - i} koment√°≈ô≈Ø.\033[0m")
                    break
                print(f"  \033[34m- {comment}\033[0m")
        else:
            print("  ≈Ω√°dn√© koment√°≈ôe nenalezeny.")

    if scan_choices['api_keys']:
        print(f"\n{os.linesep}\033[1m--- Potenci√°ln√≠ API kl√≠ƒçe/tokeny ({len(all_found_data['api_keys'])}) ---\033[0m")
        if all_found_data['api_keys']:
            print("\033[33m  Upozornƒõn√≠: Toto jsou vzory. Nemus√≠ j√≠t o aktivn√≠ kl√≠ƒçe. Pro vzdƒõl√°vac√≠ √∫ƒçely.\033[0m")
            for i, key in enumerate(sorted(list(all_found_data['api_keys']))):
                if display_limits['api_keys'] != 0 and i >= display_limits['api_keys']:
                    print(f"  \033[34m- ... a dal≈°√≠ch {len(all_found_data['api_keys']) - i} kl√≠ƒç≈Ø.\033[0m")
                    break
                print(f"  \033[34m- {key}\033[0m")
        else:
            print("  ≈Ω√°dn√© potenci√°ln√≠ API kl√≠ƒçe/tokeny nenalezeny.")

    if scan_choices['error_messages']:
        print(f"\n{os.linesep}\033[1m--- Nalezen√© chybov√© zpr√°vy ({len(all_found_data['error_messages'])}) ---\033[0m")
        if all_found_data['error_messages']:
            print("\033[33m  Upozornƒõn√≠: Nalezen√© chybov√© zpr√°vy mohou naznaƒçovat probl√©my, ale nejsou v≈ædy zranitelnost√≠.\033[0m")
            for i, msg in enumerate(sorted(list(all_found_data['error_messages']))):
                if display_limits['error_messages'] != 0 and i >= display_limits['error_messages']:
                    print(f"  \033[34m- ... a dal≈°√≠ch {len(all_found_data['error_messages']) - i} zpr√°v.\033[0m")
                    break
                print(f"  \033[34m- {msg}\033[0m")
        else:
            print("  ≈Ω√°dn√© chybov√© zpr√°vy nenalezeny.")
            
    # Identifikovan√© technologie
    print(f"\n{os.linesep}\033[1m--- Identifikovan√© Technologie ---\033[0m")
    if all_found_data['technologies']:
        for tech_name, tech_value in sorted(all_found_data['technologies'].items()):
            print(f"  \033[38;5;208m{tech_name}:\033[0m {tech_value}") # Oran≈æov√° barva
    else:
        print("  ≈Ω√°dn√© kl√≠ƒçov√© technologie nebyly identifikov√°ny.")
        print("  \033[33mTip: Zkuste prohledat zdrojov√Ω k√≥d str√°nky pro dal≈°√≠ stopy (nap≈ô. 'generator', 'version').\033[0m")
        
    # Nov√© sekce pro pokroƒçil√© OSINT
    if scan_choices['subdomains']:
        print(f"\n{os.linesep}\033[1m--- Nalezen√© Subdom√©ny ({len(all_found_data['subdomains'])}) ---\033[0m")
        if all_found_data['subdomains']:
            for i, subdomain in enumerate(sorted(list(all_found_data['subdomains']))):
                if display_limits['subdomains'] != 0 and i >= display_limits['subdomains']:
                    print(f"  \033[38;5;198m- ... a dal≈°√≠ch {len(all_found_data['subdomains']) - i} subdom√©n.\033[0m")
                    break
                print(f"  \033[38;5;198m- {subdomain}\033[0m")
        else:
            print("  ≈Ω√°dn√© subdom√©ny nenalezeny.")

    if scan_choices['reverse_ip']:
        print(f"\n{os.linesep}\033[1m--- Reverzn√≠ IP Dom√©ny ({len(all_found_data['reverse_ip_domains'])}) ---\033[0m")
        if all_found_data['reverse_ip_domains']:
            # Zobrazit IP adresu pouze pokud byla √∫spƒõ≈°nƒõ z√≠sk√°na
            if all_found_data['target_ip']:
                print(f"  \033[34mIP adresa: {all_found_data['target_ip']}\033[0m")
            else:
                print(f"  \033[34mIP adresa: Nezji≈°tƒõna\033[0m")
            
            for i, domain_name in enumerate(sorted(list(all_found_data['reverse_ip_domains']))):
                if display_limits['reverse_ip'] != 0 and i >= display_limits['reverse_ip']:
                    print(f"  \033[38;5;198m- ... a dal≈°√≠ch {len(all_found_data['reverse_ip_domains']) - i} dom√©n.\033[0m")
                    break
                print(f"  \033[38;5;198m- {domain_name}\033[0m")
        else:
            print("  ≈Ω√°dn√© dom√©ny na stejn√© IP adrese nenalezeny.")

    if scan_choices['wayback_machine']:
        print(f"\n{os.linesep}\033[1m--- Wayback Machine Arch√≠vy ({len(all_found_data['wayback_archives'])}) ---\033[0m")
        if all_found_data['wayback_archives']:
            # Se≈ôadit archivy od nejnovƒõj≈°√≠ch po nejstar≈°√≠ pro lep≈°√≠ p≈ôehled
            sorted_archives = sorted(all_found_data['wayback_archives'], key=lambda x: x['timestamp'], reverse=True)
            for i, archive in enumerate(sorted_archives):
                if display_limits['wayback_machine'] != 0 and i >= display_limits['wayback_machine']:
                    print(f"  \033[38;5;198m- ... a dal≈°√≠ch {len(all_found_data['wayback_archives']) - i} archiv≈Ø.\033[0m")
                    break
                # Form√°tov√°n√≠ ƒçasu pro lep≈°√≠ ƒçitelnost
                timestamp_str = f"{archive['timestamp'][0:4]}-{archive['timestamp'][4:6]}-{archive['timestamp'][6:8]} {archive['timestamp'][8:10]}:{archive['timestamp'][10:12]}:{archive['timestamp'][12:14]}"
                print(f"  \033[38;5;198m- Datum: {timestamp_str}, P≈Øvodn√≠ URL: {archive['original_url']}, Archiv URL: {archive['archive_url']}\033[0m")
        else:
            print("  ≈Ω√°dn√© arch√≠vy v Wayback Machine nenalezeny.")
        
    print("\n" + "=" * (os.get_terminal_size().columns - 1) if os.get_terminal_size().columns > 1 else "=")


def main():
    """Hlavn√≠ funkce pro spu≈°tƒõn√≠ OSINT Harvesteru."""
    global visited_urls
    global global_crawl_delay

    try:
        while True:
            clear_screen()
            print(ascii_logo)
            print("\n--- OSINT Harvester (Komplexn√≠ pr≈Øzkum) ---")
            print("Zadejte URL webov√© str√°nky ke skenov√°n√≠ (nap≈ô√≠klad 'https://example.com').")
            print("Pro ukonƒçen√≠ zadejte 'konec'.")
            print("\n\033[31m!!! Pamatujte na etick√© a pr√°vn√≠ z√°sady web scrapingu !!!\033[0m")
            print("\033[31m!!! Scrapujte POUZE ve≈ôejnƒõ dostupn√° data a respektujte robots.txt a ToS !!!\033[0m")
            
            target_url = input("\nZadejte c√≠lovou URL: ").strip()
            
            if target_url.lower() == 'konec':
                print("Ukonƒçuji OSINT Harvester. Na shledanou!")
                break
            
            if not target_url:
                print("C√≠lov√° URL nem≈Ø≈æe b√Ωt pr√°zdn√°. Zkuste to znovu.")
                input("\nStisknƒõte Enter pro pokraƒçov√°n√≠...")
                continue

            # Zkontrolujeme, zda URL zaƒç√≠n√° http/https
            if not target_url.startswith(('http://', 'https://')):
                target_url = 'https://' + target_url
                print(f"[*] URL upravena na: {target_url}")

            # --- Interaktivn√≠ volba typ≈Ø informac√≠ ---
            scan_choices = {
                'emails': input("Hledat e-mailov√© adresy? (a/n, v√Ωchoz√≠ a): ").lower().startswith('a') or True,
                'phones': input("Hledat telefonn√≠ ƒç√≠sla? (a/n, v√Ωchoz√≠ a): ").lower().startswith('a') or True,
                'social_links': input("Hledat odkazy na soci√°ln√≠ m√©dia? (a/n, v√Ωchoz√≠ a): ").lower().startswith('a') or True,
                'http_headers': input("Analyzovat HTTP hlaviƒçky? (a/n, v√Ωchoz√≠ a): ").lower().startswith('a') or True,
                'meta_tags': input("Analyzovat HTML meta tagy? (a/n, v√Ωchoz√≠ a): ").lower().startswith('a') or True,
                'comments': input("Hledat HTML/JS koment√°≈ôe? (a/n, v√Ωchoz√≠ a): ").lower().startswith('a') or True,
                'api_keys': input("Hledat potenci√°ln√≠ API kl√≠ƒçe/tokeny? (a/n, v√Ωchoz√≠ n): ").lower().startswith('a'),
                'error_messages': input("Hledat chybov√© zpr√°vy? (a/n, v√Ωchoz√≠ n): ").lower().startswith('a'),
                'subdomains': input("Hledat subdom√©ny (pasivnƒõ p≈ôes crt.sh)? (a/n, v√Ωchoz√≠ n): ").lower().startswith('a'),
                'reverse_ip': input("Prov√°dƒõt reverzn√≠ IP lookup (pasivnƒõ p≈ôes HackerTarget.com)? (a/n, v√Ωchoz√≠ n): ").lower().startswith('a'), # Zmƒõnƒõn n√°zev slu≈æby
                'wayback_machine': input("Hledat arch√≠vy na Wayback Machine (pasivnƒõ)? (a/n, v√Ωchoz√≠ n): ").lower().startswith('a')
            }
            
            # --- Nastaviteln√Ω poƒçet zobrazen√Ωch polo≈æek ---
            display_limits = {}
            print("\n--- Nastavte poƒçet zobrazen√Ωch polo≈æek (0 pro v≈°e, pr√°zdn√© pro v√Ωchoz√≠) ---")
            
            def get_limit_input(prompt, default_limit):
                while True:
                    try:
                        user_input = input(f"{prompt} (v√Ωchoz√≠ {default_limit}): ").strip()
                        if not user_input:
                            return default_limit
                        limit = int(user_input)
                        if limit < 0:
                            print("\033[31mPoƒçet polo≈æek nem≈Ø≈æe b√Ωt z√°porn√Ω. Zadejte pros√≠m nez√°porn√© ƒç√≠slo.\033[0m")
                        else:
                            return limit
                    except ValueError:
                        print("\033[31mNeplatn√Ω vstup. Zadejte pros√≠m ƒç√≠slo.\033[0m")

            display_limits['comments'] = get_limit_input("Zobrazit koment√°≈ôe", 5)
            display_limits['api_keys'] = get_limit_input("Zobrazit API kl√≠ƒçe/tokeny", 3)
            display_limits['error_messages'] = get_limit_input("Zobrazit chybov√© zpr√°vy", 3)
            display_limits['subdomains'] = get_limit_input("Zobrazit subdom√©ny", 10)
            display_limits['reverse_ip'] = get_limit_input("Zobrazit reverzn√≠ IP dom√©ny", 10)
            display_limits['wayback_machine'] = get_limit_input("Zobrazit arch√≠vy Wayback Machine", 5)
            
            # --- Nastaviteln√° hloubka skenov√°n√≠ ---
            while True:
                try:
                    max_depth_input = input("Zadejte maxim√°ln√≠ hloubku skenov√°n√≠ (0 pro jen hlavn√≠ str√°nku, 1 pro hlavn√≠ + 1 √∫rove≈à, atd., v√Ωchoz√≠ 1): ")
                    if not max_depth_input:
                        max_depth = 1
                    else:
                        max_depth = int(max_depth_input)
                    
                    if max_depth < 0:
                        print("\033[31mHloubka nem≈Ø≈æe b√Ωt z√°porn√°. Zadejte pros√≠m nez√°porn√© ƒç√≠slo.\033[0m")
                    else:
                        break
                except ValueError:
                    print("\033[31mNeplatn√Ω vstup. Zadejte pros√≠m ƒç√≠slo.\033[0m")

            # Resetujeme nav≈°t√≠ven√© URL a data pro ka≈æd√© nov√© skenov√°n√≠ c√≠le
            visited_urls.clear() 
            all_found_data = {
                'emails': set(),
                'phones': set(),
                'social_links': set(),
                'http_headers': {},
                'meta_tags': {},
                'comments': set(),
                'api_keys': set(),
                'error_messages': set(),
                'technologies': {},
                'subdomains': set(),
                'reverse_ip_domains': set(),
                'target_ip': None, # Ulo≈æ√≠me IP adresu c√≠le
                'wayback_archives': []
            }

            # Z√≠sk√°n√≠ dom√©ny pro extern√≠ OSINT dotazy
            parsed_target_url = urlparse(target_url)
            target_domain = parsed_target_url.netloc
            if target_domain.startswith('www.'):
                target_domain = target_domain[4:] # Odstranit www. pro lep≈°√≠ v√Ωsledky

            # Nejprve zkontrolujeme robots.txt pro hlavn√≠ URL
            check_robots_txt(target_url)

            # --- Spu≈°tƒõn√≠ extern√≠ch OSINT modul≈Ø ---
            if scan_choices['subdomains']:
                print(f"\n\033[33m[*] Z√≠sk√°v√°m subdom√©ny pro {target_domain} z crt.sh...\033[0m")
                found_subdomains = get_subdomains_from_crtsh(target_domain)
                for sd in found_subdomains:
                    all_found_data['subdomains'].add(sd)
                time.sleep(global_crawl_delay) # Prodleva po extern√≠m dotazu

            if scan_choices['reverse_ip']:
                print(f"\n\033[33m[*] Prov√°d√≠m reverzn√≠ IP lookup pro {target_domain} p≈ôes HackerTarget.com...\033[0m") # Zmƒõnƒõn n√°zev slu≈æby
                found_reverse_ip_domains = get_reverse_ip_from_hackertarget(target_domain, all_found_data) # P≈ôed√°v√°me all_found_data
                for rid in found_reverse_ip_domains:
                    all_found_data['reverse_ip_domains'].add(rid)
                time.sleep(global_crawl_delay) # Prodleva po extern√≠m dotazu

            if scan_choices['wayback_machine']:
                print(f"\n\033[33m[*] Z√≠sk√°v√°m arch√≠vy z Wayback Machine pro {target_url}...\033[0m")
                found_wayback_archives = get_wayback_machine_archives(target_url)
                all_found_data['wayback_archives'].extend(found_wayback_archives)
                time.sleep(global_crawl_delay) # Prodleva po extern√≠m dotazu


            # Pou≈æ√≠v√°me frontu pro BFS (Breadth-First Search) pro hlub≈°√≠ skenov√°n√≠
            urls_to_visit = deque([(target_url, 0)])

            print(f"\n\033[33m[*] Spou≈°t√≠m hlubok√© skenov√°n√≠ do hloubky {max_depth}...\033[0m")

            while urls_to_visit:
                current_url, current_depth = urls_to_visit.popleft()

                if current_depth > max_depth:
                    continue

                new_internal_links = get_page_content_and_info(current_url, all_found_data, scan_choices, current_depth, max_depth)
                
                if current_depth < max_depth:
                    for link in new_internal_links:
                        if link not in visited_urls:
                            urls_to_visit.append((link, current_depth + 1))
                
                print(f"\n{os.linesep}\033[33m[*] ƒåek√°m {global_crawl_delay} sekund...\033[0m")
                time.sleep(global_crawl_delay)

            display_results(all_found_data, scan_choices, display_limits)
            
            input("\nStisknƒõte Enter pro skenov√°n√≠ dal≈°√≠ URL, nebo 'konec'...")

    except KeyboardInterrupt:
        print("\nProgram byl ukonƒçen u≈æivatelem.")
    except Exception as e:
        print(f"\n\033[31m‚ùå Vyskytla se kritick√° chyba: {e}\033[0m")
    finally:
        sys.stdout.write("\033[0m\n")
        sys.stdout.flush()

if __name__ == "__main__":
    main()

